%!TEX TS-program = xelatex
%!TEX options = -aux-directory=Debug -shell-escape -file-line-error -interaction=nonstopmode -halt-on-error -synctex=1 "%DOC%"
\documentclass{article}
\input{LaTeX-Submodule/template.tex}

% Additional packages & macros
\usepackage{mdframed}

% Header and footer
\newcommand{\unitName}{High Performance and Parallel Computing}
\newcommand{\unitTime}{Semester 2, 2024}
\newcommand{\unitCoordinator}{Associate Professor Wayne Kelly}
\newcommand{\documentAuthors}{Tarang Janawalkar}

\fancyhead[L]{\unitName}
\fancyhead[R]{\leftmark}
\fancyfoot[C]{\thepage}

% Copyright
\usepackage[
    type={CC},
    modifier={by-nc-sa},
    version={4.0},
    imagewidth={5em},
    hyphenation={raggedright}
]{doclicense}

\date{}

\newenvironment{aside}[1][]
  {\begin{mdframed}[style=0,%
      leftline=false,rightline=false,leftmargin=2em,rightmargin=2em,%
          innerleftmargin=0pt,innerrightmargin=0pt,linewidth=0.75pt,%
      skipabove=7pt,skipbelow=7pt,#1]\small}
  {\end{mdframed}}

\begin{document}
%
\begin{titlepage}
    \vspace*{\fill}
    \begin{center}
        \LARGE{\textbf{\unitName}} \\[0.1in]
        \normalsize{\unitTime} \\[0.2in]
        \normalsize\textit{\unitCoordinator} \\[0.2in]
        \documentAuthors
    \end{center}
    \vspace*{\fill}
    \doclicenseThis
    \thispagestyle{empty}
\end{titlepage}
\newpage
%
\tableofcontents
\newpage
%
\section{Processor Design}
High performance programs must be designed to take advantage of the
hardware they run on. This section discusses how Central Processing
Unit (CPU) design influences how programs are executed, and why
parallelisation is necessary for high performance computing.
\subsection{Von Neumann Architecture}
Most general purpose computers are based on the von Neumann
architecture where instructions and data are kept in the same memory.
In this model, the CPU performs a fetch-decode-execute cycle to execute
instructions, as described below:
\begin{enumerate}
    \item \textbf{Fetch}: The CPU fetches the next instruction from memory into
          the instruction register and increments the program counter.
    \item \textbf{Decode}: The CPU decodes the instruction to determine what
          operation to perform. This may involve one of the following:
          \begin{itemize}
              \item Load data from memory into a register.
              \item Store data from a register into memory.
              \item Perform an arithmetic or logical operation on data
                    in registers.
              \item Transfer control to another part of the program by
                    changing the program counter.
          \end{itemize}
    \item \textbf{Execute}: The CPU performs the operation specified by
          the instruction. This may involve the Arithmetic Logic Unit
          (ALU) for arithmetic and logical operations, or the bus/memory
          module for memory operations.
\end{enumerate}
\subsection{CPU Clock}
Each instruction in the fetch-decode-execute cycle takes a certain
amount of time to complete. This time is determined by the speed of the
CPU which is controlled by a clock that generates a series of pulses at
a fixed frequency. These pulses are called \textbf{clock cycles} and
the time taken for one cycle to complete is called the \textbf{clock
period} of the CPU. This period must be long enough to:
\begin{itemize}
    \item allow electrical signals to propagate through the CPU, and
    \item allow transistors to reach a steady state when switching
          between on and off.
\end{itemize}
An increase in clock frequency therefore results in a faster CPU, but
due to the physical limitations of the speed of light and the size of
transistors inside the CPU, single-core CPU performance has plateaued
over the past decade.

Note in some cases, the CPU may be ``over-clocked'' to run at a higher
frequency than it was designed for, but this can result in incorrect
operation if the physical components are unable to switch states fast
enough, and lead to overheating.
\subsection{Von Neumann Bottleneck}
Main memory resides outside the CPU, and is connected to the CPU via a
shared bus. As the memory controller has a much lower clock frequency
than the CPU, the CPU often stalls while a data operation is being
performed between the CPU and memory. This is known as the \textbf{von
Neumann bottleneck} and is a major limitation of the von Neumann
architecture. To minimise performance loss, memory accesses should
have:
\begin{itemize}
    \item maximal throughout (i.e., transfer as much data as possible
          in one operation), and
    \item minimal latency (i.e., minimise the time taken to start and
          end an operation).
\end{itemize}
\subsubsection{Cache Memory}
A common way CPUs achieve this is by using \textbf{cache memory} to
store frequently accessed data and instructions in small specialised
blocks of memory that are close to each core.\ These caches provide
higher throughput and lower latency than main memory, but are typically
more expensive and therefore have limited capacity. A CPU often has
multiple levels of cache, starting with the Level 1 (L1) cache which is
the smallest and fastest, increasing in size and latency as the level
increases. On modern CPUs, the L1 cache is typically split into two
parts: one for \textit{instructions} and one for \textit{data}.
Multi-core CPUs may also have unified L2 and L3 caches that are shared
between cores.

\textbf{Cache Operation}\quad The cache works by loading data in blocks of a fixed size called
\textbf{cache lines}. When the CPU requests less than a cache line of
data, it loads surrounding data into the cache to take advantage of
locality of reference. This is done in the hope that the CPU will
eventually request the surrounding data, which will already be in the
cache and is therefore faster to access. When memory is requested, the
CPU first checks if it exists in the cache, requesting it from main
memory only if it is not found.

\textbf{Cache Replacement}\quad When the cache is full, the CPU must decide what data to replace in the
cache, through a \textit{cache replacement policy}. Typically this is
done using the Least Recently Used (LRU) policy, where the data that
has not been accessed for the longest time is replaced. The
\textit{cache placement policy} determines where new data is placed,
and can be:
\begin{itemize}
    \item \textbf{Fully associative}, where any cache line can be replaced.
    \item \textbf{N-way set associative}, where the cache is divided
          into N sets, and the replacement policy is applied within each set.
    \item \textbf{Direct mapped}, where new data must be mapped to a
          specific cache line.
\end{itemize}
\subsection{Instruction-Level Parallelism}
Scalar processors are designed to process and execute instructions one
at a time. High-performance processors take advantage of disjoint
operations that can be performed in parallel through
\textbf{instruction level parallelism} (ILP). This can be achieved in
several ways:
\begin{itemize}
    \item \textbf{Superscalar Execution}: Dispatching multiple
          scalar operations to different functional units (ALU, integer
          multiplication, FPU, store/load, etc.) during a single
          clock cycle. This is facilitated by hardware that determines
          dependencies between instructions and schedules them for
          parallel execution.
    \item \textbf{Out-of-Order Execution}: Executing other instructions
          while waiting for data required by the current instruction.
          As this can result in instructions being executed out of
          order, the processor must ensure that the final result is
          correct.
    \item \textbf{Instruction Pipelining}: Dividing instruction
          processing into multiple stages to keep the processor busy.
          For example the fetch-decode-execute cycle can be divided
          into three stages, with each stage being executed in
          parallel, allowing the processor to begin executing the
          fetch stage of the next instruction while the decode stage
          of the current instruction is being executed.
\end{itemize}
To keep the processor busy, processors may also employ \textbf{branch
    prediction} to predict the outcome of branch instructions and
\textbf{speculative execution} to execute instructions that may not be
required, but are likely to be executed.

Each of these techniques result more complex processor cores that
occupy more space on the CPU chip, consume more power, and have longer
cycle times.
\subsection{Processor Architectures}
There are two main types of processor architectures:
\begin{itemize}
    \item \textbf{Complex Instruction Set Computing (CISC)}: Designed
          to execute complex instructions that can perform multiple
          operations in a single instruction. These instructions may be
          decoded into micro-operations that are executed by the CPU.
          These processors have large instruction sets and typically
          have slower clock speeds.
    \item \textbf{Reduced Instruction Set Computing (RISC)}: Designed
          to execute simple instructions that perform a single
          operation. These processors have smaller instruction sets and
          typically have faster clock speeds.
\end{itemize}
\subsection{Multi-Core Processors}
\begin{aside}[frametitle={Moore's Law}]
    Moore's Law states that the number of transistors on a CPU chip doubles
    approximately every two years. This has led to:
    \begin{itemize}
        \item Smaller circuits
        \item Faster clock speeds
        \item Additional complex hardware level optimisations
    \end{itemize}
    While this has held true for the past few decades, it is now
    increasingly difficult to reduce the size of transistors due to physical
    limitations, such as the size of a wire compared to the width of an atom.
\end{aside}
Rather than allocating this additional space to a single core, CPUs now
contain multiple cores that can execute an independent instruction
stream\footnote{Note that each core may execute the same stream if each
    stream operates on different data, as in Graphical Processing Units.}.
Each core is itself a complete processor, with its own functional units,
program counter, instruction register, general-purpose registers, and
cache.

These cores can communicate with each other via shared memory, but this
leads to the same bottleneck as before. To overcome this, cores may
have their own private caches, and communicate with each other via a
shared L2 or L3 cache.
\subsubsection{Cache Coherence}
When multiple cores share the same memory, it is important to ensure
that each core has the most up-to-date copy of the data. Cache
coherence is the consistency of data stored in multiple private caches
that reference shared memory. Cache mechanisms are used to ensure that
modifications to data in private caches are propagated to other cores
to ensure other cores do not read stale data. This is typically done
using \textit{snooping} or \textit{directory-based} mechanisms:
\begin{itemize}
    \item \textbf{Snooping}: Each cache monitors the bus for
          access to memory locations that have been cached. If a write
          is detected, the cache either updates its data or invalidates
          it.
    \item \textbf{Directory-based}: A centralised directory keeps track
          of data shared between cores. Each processor must request
          permission to load an entry from a memory location. When an
          entry is updated, the directory either updates or invalidates
          other caches with that entry.
\end{itemize}
Caches also have inclusion/exclusion policies that determine whether
data in a lower-level cache may be present in a higher-level cache.
\subsection{Software Parallelism}
Parallelism can be achieved at the software level through processes and
threads. A \textbf{process} is an instance of a program being executed
by the operating system that has its own memory space and threads of
execution. Upon initialisation, a process creates a \textbf{thread}
that executes its main function. This main thread can then spawn
additional threads that execute independent streams of instructions.

A \textbf{thread} is a lightweight process that has its own program
counter and local memory called the \textbf{stack}. The lifetime of the
data in the stack is determined by the function that created it.

Every process also has a shared memory space called the \textbf{heap},
where data with indeterminate lifetimes is stored (i.e., dynamically
allocated objects). This space can be accessed by threads within the
same process, and by threads in other processes through inter-process
communication APIs.
\subsubsection{Mapping Threads to Cores}
Operating systems achieve concurrency in threads through time slicing,
where each thread is allocated a core for a fixed amount of time. When
a time slice ends, the operating system performs a context switch to
another thread by saving the current thread's state and loading the
next thread's state. This allows multiple threads to make progress on a
single core, and allows threads to be executed in parallel on systems
with multiple cores. Note that for a CPU with N cores, a process with N
threads may not always be granted all N cores, as the operating system
may allocate some cores to other processes.
\subsection{Simultaneous Multi-Threading}
To reduce the overhead incurred during a context switch, some
processors allow more than one thread to be executed on a single core
at the same time by interleaving instructions from multiple threads.
This is known as \textbf{Simultaneous Multi-Threading (SMT)} or
\textbf{Hyper-Threading}. In these processors, each core has multiple
sets of registers and program counters (typically two) for each thread,
with a shared set of functional units, that allow multiple threads to
be executed in parallel.
\section{Parallel Computing}
\subsection{Forms of Parallelism}
Parallelism can be achieved at multiple levels:
\begin{itemize}
    \item \textbf{Instruction Level Parallelism (ILP)}: Parallelism
          achieved by executing multiple instructions in parallel.
    \item \textbf{Vector Parallelism}: Parallelism achieved by
          executing the same instruction on multiple data elements
          simultaneously. This is typically done using \textbf{SIMD}
          (Single Instruction, Multiple Data) instructions.
    \item \textbf{Thread Level Parallelism (TLP)}: Parallelism achieved
          by executing multiple threads in parallel. This can be done
          using multiple cores, or by interleaving instructions from
          multiple threads on a single core.
    \item \textbf{Process Level Parallelism}: Parallelism achieved
          by executing multiple processes in parallel. Processes can
          communicate with each other through inter-process
          communication. Threads within the same process communicate via
          shared memory, while processes on different machines
          communicate via message passing. Distributed shared memory
          systems provide a shared memory programming model for
          distributed memory systems.
\end{itemize}
\subsection{Types of Parallel Computers}
\subsubsection{Flynn's Taxonomy}
Flynn's Taxonomy classifies parallel computers based on the number of
instruction streams and data streams that can be processed at the same
time. It has four categories:
\begin{itemize}
    \item \textbf{Single Instruction, Single Data (SISD)}: A single
          instruction stream is executed on a single data stream. This
          is the traditional von Neumann architecture.
    \item \textbf{Single Instruction, Multiple Data (SIMD)}: A single
          instruction stream is executed on multiple data streams
          (includes superscalar processors). This is typically done
          using vector processors or GPUs.
    \item \textbf{Multiple Instruction, Single Data (MISD)}: Multiple
          instruction streams are executed on a single data stream.
          Commonly used in fault-tolerant redundant systems.
    \item \textbf{Multiple Instruction, Multiple Data (MIMD)}: Multiple
          instruction streams are executed on multiple data streams.
          This is the most common form of parallelism, and is used in
          multi-core CPUs and distributed systems, including both
          shared memory and distributed memory systems.
\end{itemize}
\subsection{Types of Supercomputers}
\begin{itemize}
    \item \textbf{Vector Processors}: Processors that can execute
          vector instructions on multiple data elements in parallel.
          This includes early supercomputers like the Cray-1.
    \item \textbf{Symmetric Multi-Processor (SMP)}: Multi-core
          processors that share memory and have equal access to all
          resources.
    \item \textbf{Massively Parallel Processors (MPP)}: Tightly coupled
          systems with multiple processors that communicate with each
          other via proprietary high-speed interconnect.
    \item \textbf{Clusters}: Loosely coupled distributed memory systems
          that consist of commodity nodes.
    \item \textbf{Asymmetric Multi-Processor (AMP)}: Specialised
          co-processors that offload specific tasks from the main CPU (i.e., GPUs).
    \item \textbf{Hybrid Systems}: Systems that combine multiple
          architectures to take advantage of the strengths of each
          architecture.
    \item \textbf{Cycle Stealing Systems}: Systems that use idle
          resources on a network of computers to perform computations.
\end{itemize}
\subsection{Parallel Computing Models}
\subsubsection{Concurrent Computing}
A concurrent computing model is one where multiple operation streams
progress independently. This does not require multiple processors nor
does it imply simultaneous execution. Such models may be prone to
problems such as deadlocks.
\subsubsection{Parallel Computing}
A parallel computing model is one where multiple operation streams
progress simultaneously. This requires multiple processors and
simultaneous execution.
\subsubsection{Distributed Computing}
A distributed computing model is one where multiple operation streams
progress simultaneously on different machines. This includes parallel
computing clusters and distributed concurrent systems.
\subsection{Parallelisation}
Parallelisation is the process of converting a sequential program into
a parallel program, that can run on parallel hardware. Doing this
effectively requires programmers to use a fundamentally different
algorithm than the one used on sequential hardware, that is expressed
in a manner that makes parallelisation more explicit or easier.

In some cases, we can exploit parallelism without changing the
algorithm, by analysing which computational steps performed in the
algorithm can be executed in parallel. This is known as exploiting
\textbf{inherent parallelism}.
\subsubsection{Parallelisation in Functional and Imperative Programming}
\begin{itemize}
    \item \textbf{Functional Programming}: Pure functional programming
          languages express computation via function evaluation. As
          function evaluation does not produce side effects, functions
          that do not depend on the result of another can be executed in
          parallel.
    \item \textbf{Imperative Programming}: Imperative programming
          languages express computation via a sequence of statements.
          Parallelisation in imperative programming is more difficult
          as statements may have side effects that depend on the order
          of execution.
\end{itemize}
\subsection{Safe Parallelisation}
The process of parallelisation must be done in a manner that ensures
the same result is produced as the sequential program. Two dependencies
must be preserved to ensure safe parallelisation:
\begin{itemize}
    \item Control dependencies
    \item Data dependencies
\end{itemize}
\subsubsection{Control Dependences}
A control dependency is a dependency where one statement depends on
whether another statement is executed. Some examples of control
dependencies are shown below:
\begin{minted}{c}
int gcd(int a, int b) {
    // if statement
    if (b == 0) {
        return a;             // Control dependency
    } else {
        return gcd(b, a % b); // Control dependency
    }
}

int gcd(int a, int b) {
    // while loop
    while (b != 0) {
        int temp = b; // Control dependency
        b = a % b;    // Control dependency
        a = temp;     // Control dependency
    }
    return a;
}

int gcd(int a, int b) {
    // error handling
    if (a < 0 || b < 0) {
        return -1;
    }

    ... // Control dependency
}
\end{minted}
Control dependencies can be difficult to determine when the program
contains many conditional statements as the control flow depends on
input data.
\subsubsection{Data Dependencies}
A data dependency is a dependency where one statement depends on the
same data as another statement. Data dependencies can be further
classified into:
\begin{itemize}
    \item \textbf{True Dependencies (W to R)}: Where a
          statement depends on the result of a previous
          instruction.
          \begin{minted}{c}
a = 1;
b = a; // True dependency on statement 1
c = b; // True dependency on statement 1 and 2
\end{minted}
    \item \textbf{Anti-Dependence (R to W)}: Where a
          statement requires a value that is later written to.
          \begin{minted}{c}
a = 1;
b = a; // Anti-dependency on statement 3
a = 2;
\end{minted}
    \item \textbf{Output Dependence (W to W)}: Where a
          variable depends on the ordering of another write
          statement.
          \begin{minted}{c}
// Output dependency between statements 1 and 3
a = 1;
b = a;
a = 2;
\end{minted}
    \item \textbf{Input Dependence (R to R)}: Where a
          statement reads
          \begin{minted}{c}
a = 1;
b = a; // Input dependence on statement 3
c = a; // Input dependence on statement 2
\end{minted}
          Note that the order of input dependencies do not need to be
          preserved, as they do not affect the result of the program.
\end{itemize}
Data dependencies can be more challenging to determine when a program
introduces pointers or references to objects, as such aliases may
inadvertently introduce dependencies between statements. For example:
\begin{minted}{c}
T a = T::new();
T *b = &a;       // b is a reference to a

a.mutator_method();  // changes a
b->mutator_method(); // also changes a
\end{minted}
In this example, some programming languages may not explicitly show the
true dependency of the final statement.
\subsubsection{Dependency Analysis}
Dependency analysis allows us to determine whether it is safe to
reorder or parallelise statements in a program. For example, given the
following code:
\begin{minted}{c}
for (int i = 0; i < n; i++) {
    for (int j = 0; j < n; j++) {
        a[i][j + 1] = a[n][j];
    }
}
\end{minted}
we may wish to know whether there are any data dependencies between
loop iterations. That is, does there exist an iteration \(\left( i_r,\:
j_r \right)\) that reads the same array element that is written to by
iteration \(\left( i_w,\: j_w \right)\)? Mathematically,
\begin{align*}
    \exists i_r,\: j_r,\: i_w,\: j_w : {} & 0 \leqslant i_r < n \land 0 \leqslant j_r < n \land {} \\
                                          & 0 \leqslant i_w < n \land 0 \leqslant j_w < n \land {} \\
                                          & i_w = n \land j_w + 1 = j_r.
\end{align*}
Another way to analyse a loop would be to draw an iteration space
diagram, with one axis representing the iteration number and the other
representing the array index. This allows us to visualise the
dependencies between iterations.

Unfortunately, any form of static dependency analysis is inexact in
general.
\begin{itemize}
    \item Alias analysis is undecidable
    \item Data dependence analysis are undecidable
\end{itemize}
When in doubt, we must assume that a dependency exists, and therefore
cannot parallelise the code.
\subsection{Automatic Parallelisation}
Automatic parallelisation can be performed either
\begin{itemize}
    \item Automatically by the compiler, or
    \item Manually by the programmer.
\end{itemize}
Current compilers are not smart enough to perform parallelisation in
general as they are necessarily conservative in their analysis. On the
other hand, manual parallelisation requires competence and can be very
time-consuming and error-prone.
\subsection{Parallelisation Process}
Generally, we can break the process of parallelisation into three
steps:
\begin{enumerate}
    \item Determine what can be safely parallelised. This may involve:
          \begin{itemize}
              \item Analysing control and data dependencies.
              \item Transforming the program to expose parallelism.
              \item Employing a more efficient algorithm that is better
                    suited for parallelism.
          \end{itemize}
    \item Decide if parallelism will increase performance:
          \begin{itemize}
              \item Is a significant amount of time spent in that code?
              \item Is there a overhead associated with creating and
                    managing threads?
              \item Will the latency and/or throughput of communication
                    between processors be a bottleneck?
              \item Does parallelisation actually result in a speedup?
          \end{itemize}
    \item Transform the program into an explicitly parallel form:
          \begin{itemize}
              \item Use programming language constructs to map
                    computation (and possibly data) to processors and
                    appropriate synchronisation.
          \end{itemize}
\end{enumerate}
\subsection{Parallelism Granularity}
While a potential to parallelise code may exist, executing this code
has an associated overhead due to the costs of creating and managing
threads, the synchronisation of parallel computations, and the latency
of messages sent between processors. Therefore the amount of
computation in each ``work unit'' must be sufficiently large to offset
this overhead.

Here we can introduce the concept of \textbf{parallelism granularity},
which is the size of the work unit that is executed in parallel.
Generally,
\begin{itemize}
    \item \textbf{Coarse-grained parallelism} has large work units that
          are executed in parallel. Distributed systems and clusters
          are suited for coarse-grained parallelism.
    \item \textbf{Fine-grained parallelism} has small work units that
          are executed in parallel. CPUs with vector computation units
          are suited for fine-grained parallelism.
\end{itemize}
\subsection{Speedup}
The speedup of a parallel program is the ratio of the time taken to
execute the best sequential program to the time taken to execute the
parallel program:
\begin{equation}
    S = \frac{\text{execution time of best sequential program}}{\text{execution time of parallel program}}
\end{equation}
Note that the best sequential program is not the same as the parallel
program restricted to a single processor, due to the aforementioned
overheads. Rather, it is the best possible sequential program that can
be written for the given problem, which may use different algorithms.
\subsubsection{Speedup Curves}
Commonly, we will plot the speedup of a parallel program against the
number of processors used. The resulting curves can be classified into
five categories:
\begin{itemize}
    \item \textbf{Super-Linear Speedup}: The speedup increases faster
          than the number of processors used. This typically only occurs
          in extreme cases where the parallel program runs on different
          hardware that allows it to outperform the sequential program.
    \item \textbf{Linear Speedup}: The speedup increases linearly with
          the number of processors used. This is the ideal case, but is
          rarely achieved due to overheads associated with parallelism.
    \item \textbf{Sub-Linear Speedup}: The speedup increases slower than
          the number of processors used. This is the most common case,
          and often the speedup plateaus as the number of processors
          increases.
    \item \textbf{No Speedup}: The speedup remains constant as the
          number of processors used increases. This indicates that the
          program likely is not parallelisable.
    \item \textbf{Slowdown}: The speedup decreases below 1 as the number
          of processors used increases. This indicates that the overhead
          of parallelism is greater than the benefits of parallelism.
\end{itemize}
\subsubsection{Scalable Parallelism}
Typically an increase in problem size results in an increase in total
execution time. In such cases, we want large problems to provide more
potential for parallelism. A problem is said to be \textbf{scalable} if
the speedup of the parallel program increases with the problem size,
without plateauing.
\subsubsection{Computational Complexity}
While we may be able to achieve scalable parallelism, it is important
to note that the computational complexity of the problem does not
change. This is because the computational complexity of a problem is
determined by the size of the problem, and distributing the problem
across finitely many processors does not change the limiting behaviour
of the problem.
\begin{equation*}
    \mathcal{O}\left( n \log{\left( n \right)} / c \right) = \mathcal{O}\left( n \log{\left( n \right)} \right)
\end{equation*}
(\(n\) is the size of the problem and \(c\) is the number of
processors used).
\subsubsection{Amdahl's Law}
Amdahl's Law states that the speedup of a parallel program is limited
by the fraction of the program that cannot be parallelised. If \(p\) is
the fraction of the program that can be parallelised, then the
theoretical speedup \(S\) of the execution of the program is given by:
\begin{equation}
    S = \frac{1}{\left( 1 - p \right) + p/s}
\end{equation}
where \(s\) is the speedup of the parallelisable portion of the program.
\subsection{Parallelisation Methodology}
When we want to parallelise a program, we can take the following steps:
\begin{enumerate}
    \item Obtain representative and realistic data sets.
    \item Time and profile the sequential version of the program.
    \item View the source code and understand the high-level structure
          of the program.
    \item Analyse dependencies.
    \item Determine sections that can be parallelised.
    \item Decide what parallelism might be worth exploiting.
    \item Consider restructuring the program or replacing algorithms to
          expose more parallelism.
    \item Transform the program into an explicitly parallel form.
    \item Test and debug the parallel version of the program.
    \item Time and profile the parallel version of the program.
    \item Determine issues inhibiting greater performance.
\end{enumerate}
\subsubsection{Timing and Profiling}
When timing an application, we must:
\begin{itemize}
    \item Time a variety of data sets.
    \item Time multiple times to get an average.
    \item Be aware of other applications running in the background.
    \item Compare performance under identical conditions.
    \item Eliminate any unnecessary screen I/O by redirecting outputs
          to a file.
    \item Use high resolution timers.
    \item Distinguish between timing the entire program and timing
          specific sections.
    \item Distinguish between real time (wall clock time) and CPU time
          (time actually spent executing instructions in the program).
    \item Be aware of one-off costs (e.g., just-in-time
          compilation/dynamic library loading).
\end{itemize}
Profiling is the process of analysing the performance of a program by
measuring the time spent in each section of the program. Profiling can
capture time spent executing a section of code and the number of times
that section is executed. Profiling can be done through sampling where
the program counter is probed at regular intervals, or through
instrumentation where the program is modified to record the time spent
in each section. It is important to note that profiling can introduce
overheads that may affect the performance of the program. Additionally,
some compilers may allow the programmer to optimise a program aggressively
so that the profiled program may not be representative of the actual
program (i.e., sections may be optimised out or reordered).
\subsection{Premature Optimisation}
\begin{aside}
    Roughly 80\% of effects come from 20\% of causes.

    \raggedleft{\textbf{Pareto Principle}}
\end{aside}
\begin{quote}
    ``There is no doubt that the grail of efficiency leads to abuse.
    Programmers waste enormous amounts of time thinking about, or worrying
    about, the speed of noncritical parts of their programs, and these
    attempts at efficiency actually have a strong negative impact when
    debugging and maintenance are considered. We should forget about small
    efficiencies, say about 97\% of the time: premature optimization is the
    root of all evil.

    Yet we should not pass up our opportunities in that critical 3\%. A good
    programmer will not be lulled into complacency by such reasoning, he
    will be wise to look carefully at the critical code; but only after that
    code has been identified. It is often a mistake to make a priori
    judgments about what parts of a program are really critical, since the
    universal experience of programmers who have been using measurement
    tools has been that their intuitive guesses failed.''

    \raggedleft{\textbf{Donald Knuth}}
\end{quote}
\end{document}
